{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# langchain 常用组件：\n",
    "- 模型输入/输出（Model I/O）：与语言模型交互的接口\n",
    "- 数据连接（Data connection）：与特定应用程序的数据进行交互的接\n",
    "- 链(Chains)：将组件组合实现端到端应用。\n",
    "- 记忆（Memory）：用于链的多次运行之间持久化应用程序状态；\n",
    "- 索引(Indexes)：提供数据检索功能。\n",
    "- 代理(Agents)：扩展模型的推理能力，用于复杂的应用的调用序列；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 模型输入/输出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain 中模型输入/输出模块是与各种大语言模型进行交互的基本组件，是大语言模型应用的核心元素。**模型 I/O 允许您管理 prompt（提示），通过通用接口调用语言模型以及从模型输出中提取信息**。该模块的基本流程如下图所示。\n",
    "\n",
    "  \n",
    "![](../Tutorial_for_developing_LLM_application-main/Tutorial_for_developing_LLM_application-main/figures/langchain_model_input_output.png)\n",
    "\n",
    "主要包含以下部分：`Prompts`、`Language Models`以及 `Output Parsers`。**用户原始输入与模型和示例进行组合，然后输入给大语言模型，再根据大语言模型的返回结果进行输出或者结构化处理**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 模型\n",
    "从langchain.chat_models导入OpenAI的对话模型ChatOpenAI。 除去OpenAI以外，langchain.chat_models还集成了其他对话模型，更多细节可以查看 [Langchain 官方文档](https://python.langchain.com/en/latest/modules/models/chat/integrations.html)(https://python.langchain.com/en/latest/modules/models/chat/integrations.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# 读取本地/项目的环境变量。\n",
    "\n",
    "# find_dotenv()寻找并定位.env文件的路径\n",
    "# load_dotenv()读取该.env文件，并将其中的环境变量加载到当前的运行环境中  \n",
    "# 如果你设置的是全局的环境变量，这行代码则没有任何作用。\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# 获取环境变量 OPENAI_API_KEY\n",
    "openai.api_key = os.environ['OPENAI_API_KEY'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.0, openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy='')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "chat = ChatOpenAI(temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 使用提示模版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 中文提示\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# 定义提示模版\n",
    "template_string = \"\"\"把由三个反引号分隔的文本\\\n",
    "翻译成一种{style}风格。\\\n",
    "文本: ```{text}```\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "customer_style = \"\"\"正式普通话 \\\n",
    "用一个平静、尊敬的语气\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "阿，我很生气，\\\n",
    "因为我的搅拌机盖掉了，\\\n",
    "把奶昔溅到了厨房的墙上！\\\n",
    "更糟糕的是，保修不包括打扫厨房的费用。\\\n",
    "我现在需要你的帮助，伙计！\n",
    "\"\"\"\n",
    "\n",
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)\n",
    "\n",
    "\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "尊敬的伙计，我感到非常愤怒，因为我的搅拌机盖子不慎掉落，导致奶昔溅到了厨房的墙壁上！更加令人糟心的是，保修并不包括厨房清洁的费用。现在，我需要你的帮助，请你给予援手！\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 输出解析器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"礼物\": string  // 这件物品是作为礼物送给别人的吗？                            如果是，则回答 是的，                            如果否或未知，则回答 不是。\n",
      "\t\"交货天数\": string  // 产品需要多少天才能到达？                                      如果没有找到该信息，则输出-1。\n",
      "\t\"价钱\": string  // 提取有关价值或价格的任何句子，                                    并将它们输出为逗号分隔的 Python 列表\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "review_template_2 = \"\"\"\\\n",
    "对于以下文本，请从中提取以下信息：：\n",
    "\n",
    "礼物：该商品是作为礼物送给别人的吗？\n",
    "如果是，则回答 是的；如果否或未知，则回答 不是。\n",
    "\n",
    "交货天数：产品到达需要多少天？ 如果没有找到该信息，则输出-1。\n",
    "\n",
    "价钱：提取有关价值或价格的任何句子，并将它们输出为逗号分隔的 Python 列表。\n",
    "\n",
    "文本: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"礼物\",\n",
    "                             description=\"这件物品是作为礼物送给别人的吗？\\\n",
    "                            如果是，则回答 是的，\\\n",
    "                            如果否或未知，则回答 不是。\")\n",
    "\n",
    "delivery_days_schema = ResponseSchema(name=\"交货天数\",\n",
    "                                      description=\"产品需要多少天才能到达？\\\n",
    "                                      如果没有找到该信息，则输出-1。\")\n",
    "\n",
    "price_value_schema = ResponseSchema(name=\"价钱\",\n",
    "                                    description=\"提取有关价值或价格的任何句子，\\\n",
    "                                    并将它们输出为逗号分隔的 Python 列表\")\n",
    "\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"礼物\": false,\n",
      "\t\"交货天数\": \"两天后\",\n",
      "\t\"价钱\": \"它比其他吹叶机稍微贵一点\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat(messages)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'礼物': False, '交货天数': '两天后', '价钱': '它比其他吹叶机稍微贵一点'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dict = output_parser.parse(response.content)\n",
    "output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大语言模型(Large Language Model, LLM), 比如 ChatGPT , 可以回答许多不同的问题。**但是大语言模型的知识来源于其训练数据集，并没有用户的信息（比如用户的个人数据，公司的自有数据），也没有最新发生时事的信息（在大模型数据训练后发表的文章或者新闻）**。因此大模型能给出的答案比较受限。如果能够让大模型在训练数据集的基础上，利用我们自有数据中的信息来回答我们的问题，那便能够得到更有用的答案。\n",
    "\n",
    "为了支持上述应用的构建，LangChain 数据连接（Data connection）模块通过以下方式提供组件来**加载、转换、存储和查询数据**：`Document loaders`、`Document transformers`、`Text embedding models`、`Vector stores` 以及 `Retrievers`。数据连接模块部分的基本框架如下图所示。\n",
    "\n",
    "![](../Tutorial_for_developing_LLM_application-main/Tutorial_for_developing_LLM_application-main/figures/data_collection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 链（Chain）\n",
    "虽然独立使用大型语言模型能够应对一些简单任务，但**对于更加复杂的需求，可能需要将多个大型语言模型进行链式组合，或与其他组件进行链式调用**。链允许将多个组件组合在一起，创建一个单一的、连贯的应用程序。例如，可以创建一个链，接受用户输入，使用 PromptTemplate 对其进行格式化，然后将格式化后的提示词传递给大语言模型。也可以通过将多个链组合在一起或将链与其他组件组合来构建更复杂的链。\n",
    "\n",
    "大语言模型链（LLMChain）是一个简单但非常强大的链，也是后面我们将要介绍的许多链的基础。我们以它为例，进行介绍：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI \n",
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from langchain.chains import LLMChain  \n",
    "\n",
    "# 这里我们将参数temperature设置为0.0，从而减少生成答案的随机性。\n",
    "# 如果你想要每次得到不一样的有新意的答案，可以尝试调整该参数。\n",
    "llm = ChatOpenAI(temperature=0.0)  \n",
    "\n",
    "#初始化提示模版\n",
    "prompt = ChatPromptTemplate.from_template(\"描述制造{product}的一个公司的最佳名称是什么?\")\n",
    "\n",
    "#将大语言模型(LLM)和提示（Prompt）组合成链\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "#运行大语言模型链\n",
    "product = \"大号床单套装\"\n",
    "chain.run(product)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了上例中给出的 LLMChain，LangChain 中链还包含 RouterChain、SimpleSequentialChain、SequentialChain、TransformChain 等。\n",
    "\n",
    "- `RouterChain` 可以**根据输入数据的某些属性/特征值，选择调用不同的子链**（Subchain）。\n",
    "- `SimpleSequentialChain` 是最简单的序列链形式，其中每个步骤具有单一的输入/输出，**上一个步骤的输出是下一个步骤的输入**。\n",
    "- `SequentialChain` 是简单顺序链的更复杂形式，允许**多个输入/输出**。\n",
    "- `TransformChain` 可以引入**自定义转换函数，对输入进行处理后进行输出**。\n",
    "\n",
    "以下是使用 SimpleSequentialChain 的代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "\n",
    "#创建两个子链\n",
    "\n",
    "# 提示模板 1 ：这个提示将接受产品并返回最佳名称来描述该公司\n",
    "first_prompt = ChatPromptTemplate.from_template(   \n",
    "    \"描述制造{product}的一个公司的最好的名称是什么\"\n",
    ")\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "# 提示模板 2 ：接受公司名称，然后输出该公司的长为20个单词的描述\n",
    "second_prompt = ChatPromptTemplate.from_template(   \n",
    "    \"写一个20字的描述对于下面这个\\\n",
    "    公司：{company_name}的\"\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "#构建简单顺序链\n",
    "#现在我们可以组合两个LLMChain，以便我们可以在一个步骤中创建公司名称和描述\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "\n",
    "\n",
    "#运行简单顺序链\n",
    "product = \"大号床单套装\"\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 记忆（Meomory）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 LangChain 中，记忆（Memory）指的是大语言模型（LLM）的短期记忆。为什么是短期记忆？那是因为LLM训练好之后 (获得了一些长期记忆)，它的参数便不会因为用户的输入而发生改变。当用户与训练好的LLM进行对话时，LLM 会暂时记住用户的输入和它已经生成的输出，以便预测之后的输出，而模型输出完毕后，它便会“遗忘”之前用户的输入和它的输出。因此，之前的这些信息只能称作为 LLM 的短期记忆。\n",
    "\n",
    "- 对话缓存储存 (ConversationBufferMemory）\n",
    "- 对话缓存窗口储存 (ConversationBufferWindowMemory）\n",
    "- 对话令牌缓存储存 (ConversationTokenBufferMemory）\n",
    "- 对话摘要缓存储存 (ConversationSummaryBufferMemory）\n",
    "\n",
    "![](../Tutorial_for_developing_LLM_application-main/Tutorial_for_developing_LLM_application-main/figures/memory.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0)  \n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(llm=llm, memory = memory, verbose=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 第一轮对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"你好, 我叫皮皮鲁\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 第二轮对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\n",
      "Human: 1+1等于多少？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1等于2。'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"1+1等于多少？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 第三轮对话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: 你好, 我叫皮皮鲁\n",
      "AI: 你好，皮皮鲁！很高兴认识你。我是一个AI助手，可以回答你的问题和提供帮助。有什么我可以帮你的吗？\n",
      "Human: 1+1等于多少？\n",
      "AI: 1+1等于2。\n",
      "Human: What is my name?\n",
      "AI: 你的名字是皮皮鲁。\n",
      "Human: 我叫什么名字？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'你叫皮皮鲁。'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation.predict(input=\"我叫什么名字？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 代理（Agents）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**大型语言模型（LLMs）非常强大，但它们缺乏“最笨”的计算机程序可以轻松处理的特定能力**。LLM 对逻辑推理、计算和检索外部信息的能力较弱，这与最简单的计算机程序形成对比。例如，语言模型无法准确回答简单的计算问题，还有当询问最近发生的事件时，其回答也可能过时或错误，因为无法主动获取最新信息。这是由于当前语言模型仅依赖预训练数据，与外界“断开”。要克服这一缺陷， LangChain 框架提出了 “代理”( Agent ) 的解决方案。**代理作为语言模型的外部模块，可提供计算、逻辑、检索等功能的支持，使语言模型获得异常强大的推理和获取信息的超能力**。\n",
    "\n",
    "![](./agents.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools(\n",
    "    [\"llm-math\",\"wikipedia\"], \n",
    "    llm=llm #第一步初始化的模型\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  加载工具包\n",
    "- `llm-math` 工具结合语言模型和计算器用以进行数学计算\n",
    "- `wikipedia`工具通过API连接到wikipedia进行搜索查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools(\n",
    "    [\"llm-math\",\"wikipedia\"], \n",
    "    llm=llm #第一步初始化的模型\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3️⃣ 初始化代理\n",
    "\n",
    "- `agent`: 代理类型。这里使用的是`AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION`。其中`CHAT`代表代理模型为针对对话优化的模型，`REACT`代表针对REACT设计的提示模版。\n",
    "- `handle_parsing_errors`: 是否处理解析错误。当发生解析错误时，将错误信息返回给大模型，让其进行纠正。\n",
    "- `verbose`: 是否输出中间步骤结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent= initialize_agent(\n",
    "    tools, #第二步加载的工具\n",
    "    llm, #第一步初始化的模型\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,  #代理类型\n",
    "    handle_parsing_errors=True, #处理解析错误\n",
    "    verbose = True #输出中间步骤\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: 计算300的25%\n",
      "Thought: 我可以使用计算器来计算这个百分比\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"300 * 25 / 100\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnswer: 75.0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道最终答案了\n",
      "Final Answer: 300的25%是75.0\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': '计算300的25%，请用中文', 'output': '300的25%是75.0'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent(\"计算300的25%，思考过程请使用中文。\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **总结**\n",
    "\n",
    "1. 模型对于接下来需要做什么，给出思考（Thought） \n",
    "    \n",
    "   <p style=\"font-family:verdana; font-size:12px;color:green\"> <strong>思考</strong>：我们需要计算300的25%，这个过程中需要用到乘法和除法。</p>\n",
    "\n",
    "2. 模型基于思考采取行动（Action）\n",
    "    <p style=\"font-family:verdana; font-size:12px;color:green\"> <strong>行动</strong>: 使用计算器（calculator），输入300*0.25</p>\n",
    "3. 模型得到观察（Observation）\n",
    "    <p style=\"font-family:verdana; font-size:12px;color:green\"><strong>观察</strong>：答案: 75.0</p>\n",
    "\n",
    "4. 基于观察，模型对于接下来需要做什么，给出思考（Thought）\n",
    "    <p style=\"font-family:verdana; font-size:12px;color:green\"> <strong>思考</strong>: 我们的问题有了答案 </p>\n",
    "\n",
    "5. 给出最终答案（Final Answer）\n",
    "     <p style=\"font-family:verdana; font-size:12px;color:green\"> <strong>最终答案</strong>: 75.0 </p>\n",
    "5. 以字典的形式给出最终答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.回调（Callback）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain提供了一个**回调系统，允许您连接到LLM应用程序的各个阶段。这对于日志记录、监视、流式处理和其他任务非常有用**。\n",
    "\n",
    "**Callback 模块扮演着记录整个流程运行情况的角色，充当类似于日志的功能。在每个关键节点，它记录了相应的信息，以便跟踪整个应用的运行情况**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战：基于本地知识库的问答助手"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在接下来的内容里，我们将使用搭建好的向量数据库，对 query 查询问题进行召回，并将召回结果和 query 结合起来构建 prompt，输入到大模型中进行问答。   \n",
    "![](./chat_use_case-eb8a4883931d726e9f23628a0d22e315.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 加载向量数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings    # 调用 OpenAI 的 Embeddings 模型\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "\n",
    "#import panel as pn # GUI\n",
    "# pn.extension()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从环境变量中加载你的 OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载向量数据库，其中包含了 knowledge_base 下多个文档的 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Embeddings\n",
    "embedding = OpenAIEmbeddings() \n",
    "\n",
    "# 向量数据库持久化路径\n",
    "persist_directory = '../knowledge_base/chroma'\n",
    "\n",
    "# 加载数据库\n",
    "vectordb = Chroma(\n",
    "    persist_directory=persist_directory,  # 允许我们将persist_directory目录保存到磁盘上\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量库中存储的数量：1121\n"
     ]
    }
   ],
   "source": [
    "print(f\"向量库中存储的数量：{vectordb._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以测试一下加载的向量数据库，使用一个问题 query 进行向量检索。如下代码会在向量数据库中根据相似性进行检索，返回前 k 个最相似的文档。\n",
    "\n",
    "> ⚠️使用相似性搜索前，请确保你已安装了 OpenAI 开源的快速分词工具 tiktoken 包：`pip install tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的内容数：3\n"
     ]
    }
   ],
   "source": [
    "question = \"什么是强化学习\"\n",
    "docs = vectordb.similarity_search(question,k=3)\n",
    "print(f\"检索到的内容数：{len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印一下检索到的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检索到的第0个内容: \n",
      " B站的小伙伴们好\n",
      "\n",
      "我是蘑菇书一语二语二强化学习教程的作者之一王奇\n",
      "\n",
      "今天来有给大家带来一个强化学习的入门指南\n",
      "\n",
      "本次入门指南基于蘑菇书一语二语二强化学习教程\n",
      "\n",
      "本书的作者目前都是Dell会员成员\n",
      "\n",
      "也都是数学在读\n",
      "\n",
      "下面去介绍每个作者\n",
      "\n",
      "我是王奇\n",
      "\n",
      "目前就留于中国科研院大学\n",
      "\n",
      "引用方向是深度学习、静态视觉以及数据挖掘\n",
      "\n",
      "杨玉云目前就读于清华大学\n",
      "\n",
      "他的引用方向为\n",
      "\n",
      "时空数据挖掘、智能冲砍系统以及\n",
      "--------------\n",
      "检索到的第1个内容: \n",
      " 而人工智能的基本挑战是\n",
      "\n",
      "学习在不确定的情况下做出好的决策\n",
      "\n",
      "这边我举个例子\n",
      "\n",
      "比如你想让一个小孩学会走路\n",
      "\n",
      "他就需要通过不断尝试来发现\n",
      "\n",
      "怎么走比较好\n",
      "\n",
      "怎么走比较快\n",
      "\n",
      "强化学习的交互过程可以通过这张图来表示\n",
      "\n",
      "强化学习由智能体和环境两部分组成\n",
      "\n",
      "在强化学习过程中\n",
      "\n",
      "智能体与环境一直在交互\n",
      "\n",
      "智能体在环境中获取某个状态后\n",
      "\n",
      "它会利用刚刚的状态输出一个动作\n",
      "\n",
      "这个动作也被称为决策\n",
      "\n",
      "然后这个动作会\n",
      "--------------\n",
      "检索到的第2个内容: \n",
      " 围棋游戏中比较出名的一个\n",
      "\n",
      "强化学习的算法就是AlphaGo\n",
      "\n",
      "此外我们可以使用强化学习\n",
      "\n",
      "来控制机器人\n",
      "\n",
      "以及来实现助力交通\n",
      "\n",
      "另外还可以使用强化学习\n",
      "\n",
      "来更好地给我们做推进\n",
      "\n",
      "接下来就到第二部分\n",
      "\n",
      "也就是为什么要使用本书来学习强化学习\n",
      "\n",
      "这部分其实也是讲\n",
      "\n",
      "这个蘑菇书它出版的一些故事\n",
      "\n",
      "当时我在学习强化学习的时候\n",
      "\n",
      "搜集了一些资料\n",
      "\n",
      "然后我发现这些资料\n",
      "\n",
      "都有点灰色难懂\n",
      "\n",
      "并不是那么容易地上手\n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(docs):\n",
    "    print(f\"检索到的第{i}个内容: \\n {doc.page_content[:200]}\", end=\"\\n--------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 创建一个 LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们调用 OpenAI 的 API 创建一个 LLM，当然你也可以使用其他 LLM 的 API 进行创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！有什么我可以帮助你的吗？'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name = \"gpt-3.5-turbo\", temperature = 0 )\n",
    "llm.predict(\"你好\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 构建 prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n",
    "#             如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分。\n",
    "#             答案请使用中文。\n",
    "#             总是在回答的最后说“谢谢你的提问！”。\n",
    "# 已知信息：{context}\n",
    "# 问题: {question}\"\"\"\n",
    "template = \"\"\"使用以下上下文来回答最后的问题。如果你不知道答案，就说你不知道，不要试图编造答\n",
    "案。尽量最多使用三句话。使答案简明扼要。总是在回答的最后说“谢谢你的提问！”。\n",
    "{context}\n",
    "问题: {question}\n",
    "有用的回答:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\",\"question\"],\n",
    "                                 template=template)\n",
    "\n",
    "# 运行 chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再创建一个基于模板的检索链："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,\n",
    "                                       retriever=vectordb.as_retriever(),\n",
    "                                       return_source_documents=True,\n",
    "                                       chain_type_kwargs={\"prompt\":QA_CHAIN_PROMPT})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建检索 QA 链的方法 RetrievalQA.from_chain_type() 有如下参数：\n",
    "- llm：指定使用的 LLM\n",
    "- 指定 chain type : RetrievalQA.from_chain_type(chain_type=\"map_reduce\")，也可以利用load_qa_chain()方法指定chain type。\n",
    "- 自定义 prompt ：通过在RetrievalQA.from_chain_type()方法中，指定chain_type_kwargs参数，而该参数：chain_type_kwargs = {\"prompt\": PROMPT}\n",
    "- 返回源文档：通过RetrievalQA.from_chain_type()方法中指定：return_source_documents=True参数；也可以使用RetrievalQAWithSourceChain()方法，返回源文档的引用（坐标或者叫主键、索引）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 效果测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"什么是南瓜书？\"\n",
    "question_2 = \"王阳明是谁？\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 基于召回结果和 query 结合起来构建的 prompt 效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_1 的结果：\n",
      "南瓜书是对《机器学习》（西瓜书）中难以理解的公式进行解析和补充推导细节的一本书。谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question_1})\n",
    "print(\"大模型+知识库后回答 question_1 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "大模型+知识库后回答 question_2 的结果：\n",
      "我不知道王阳明是谁，谢谢你的提问！\n"
     ]
    }
   ],
   "source": [
    "result = qa_chain({\"query\": question_2})\n",
    "print(\"大模型+知识库后回答 question_2 的结果：\")\n",
    "print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 大模型自己回答的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"南瓜书是指《深入理解计算机系统》（Computer Systems: A Programmer's Perspective）一书的俗称。这本书是由Randal E. Bryant和David R. O'Hallaron合著的计算机科学教材，旨在帮助读者深入理解计算机系统的工作原理和底层机制。南瓜书因其封面上有一个南瓜图案而得名，被广泛用于大学的计算机科学和工程课程中。\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = \"\"\"请回答下列问题:\n",
    "                            {}\"\"\".format(question_1)\n",
    "\n",
    "### 基于大模型的问答\n",
    "llm.predict(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'王阳明（1472年-1529年），字仲明，号阳明子，是明代中期著名的思想家、政治家、军事家和教育家。他提出了“心即理”、“知行合一”的思想，强调人的内心自觉和道德修养的重要性。他的思想对中国历史产生了深远的影响，被后世尊称为“阳明先生”。'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt_template = \"\"\"请回答下列问题:\n",
    "                            {}\"\"\".format(question_2)\n",
    "\n",
    "### 基于大模型的问答\n",
    "llm.predict(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⭐ 通过以上两个问题，我们发现 LLM 对于一些近几年的知识以及非常识性的专业问题，回答的并不是很好。而加上我们的本地知识，就可以帮助 LLM 做出更好的回答。另外，也有助于缓解大模型的“幻觉”问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实战：文档生成总结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo-16k-0613\", openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_BASE_URL)\n",
    "\n",
    "template = \"\"\"\n",
    "## Input\n",
    "{text}\n",
    "\n",
    "## Instruction\n",
    "Please summarize the piece of text in the input part above.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "# 创建一个Output Parser，包含两个输出字段，并指定类型和说明\n",
    "output_parser = StructuredOutputParser.from_response_schemas(\n",
    "    [\n",
    "        ResponseSchema(name=\"keywords\", type=\"list\", description=\"keywords of the text\"),\n",
    "        ResponseSchema(name=\"summary\", type=\"string\", description=\"summary of the text\"),\n",
    "    ]\n",
    ")\n",
    "# 创建Prompt Template，并将format_instructions通过partial_variables直接指定为Output Parser的format\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")\n",
    "# 创建Chain并绑定Prompt Template和Output Parser(它将自动使用Output Parser解析llm输出)\n",
    "summarize_chain = LLMChain(llm=llm, verbose=True, prompt=prompt, output_parser=output_parser)\n",
    "\n",
    "to_summarize_text = 'Abstract. Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large.'\n",
    "output = summarize_chain.predict(text=to_summarize_text)\n",
    "\n",
    "import json\n",
    "print (json.dumps(output, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xuhu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
